{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a93cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c62076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed, so we can get the same results after rerunning several times\n",
    "np.random.seed(314)\n",
    "tf.random.set_seed(314)\n",
    "random.seed(314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b42e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    # shuffle two arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
    "            to False will split datasets in a random way\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
    "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    if split_by_date:\n",
    "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"]  = X[train_samples:]\n",
    "        result[\"y_test\"]  = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:    \n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c29a09a",
   "metadata": {},
   "source": [
    "# This function is long but handy, it accepts several arguments to be as flexible as possible:\n",
    "\n",
    "The ticker argument is the ticker we want to load, for instance, you can use TSLA for the Tesla stock market, AAPL for Apple, and so on. It can also be a pandas Dataframe with the condition it includes the columns in feature_columns as well as date as an index.\n",
    "\n",
    "n_steps integer indicates the historical sequence length we want to use, some people call it the window size, recall that we are going to use a recurrent neural network, we need to feed into the network a sequence data, choosing 50 means that we will use 50 days of stock prices to predict the next lookup time step.\n",
    "\n",
    "scale is a boolean variable that indicates whether to scale prices from 0 to 1, we will set this to True as scaling high values from 0 to 1 will help the neural network to learn much faster and more effectively.\n",
    "lookup_step is the future lookup step to predict, the default is set to 1 (e.g next day). 15 means the next 15 days, and so on.\n",
    "\n",
    "split_by_date is a boolean which indicates whether we split our training and testing sets by date, setting it to False means we randomly split the data into training and testing using sklearn's train_test_split() function. If it's True (the default), we split the data in date order.\n",
    "\n",
    "We will be using all the features available in this dataset, which are the open, high, low, volume and adjusted close. Please check this tutorial to learn more about what these indicators are.\n",
    "\n",
    "# The above function does the following:\n",
    "\n",
    "First, it loads the dataset using stock_info.get_data() function in yahoo_fin module.\n",
    "\n",
    "It adds the \"date\" column from the index if it doesn't exist, this will help us later to get the features of the testing set.\n",
    "\n",
    "If the scale argument is passed as True, it will scale all the prices from 0 to 1 (including the volume) using the sklearn's MinMaxScaler class. Note that each column has its own scaler.\n",
    "\n",
    "It then adds the future column which indicates the target values (the labels to predict, or the y's) by shifting the adjusted close column by lookup_step.\n",
    "\n",
    "After that, it shuffles and splits the data into training and testing sets, and finally returns the result.\n",
    "\n",
    "To understand the code even better, I highly suggest you manually print the output variable (result) and see how the features and labels are made.\n",
    "\n",
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a954690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c514e725",
   "metadata": {},
   "source": [
    "Again, this function is flexible too, you can change the number of layers, dropout rate, the RNN cell, loss, and the optimizer used to compile the model.\n",
    "\n",
    "The above function constructs an RNN that has a dense layer as output layer with 1 neuron, this model requires a sequence of features of sequence_length (in this case, we will pass 50 or 100) consecutive time steps (which are days in this dataset) and outputs a single value which indicates the price of the next time step.\n",
    "\n",
    "It also accepts n_features as an argument, which is the number of features we will pass on each sequence, in our case, we'll pass adjclose, open, high, low and volume columns (i.e 5 features).\n",
    "\n",
    "You can tweak the default parameters as you wish, n_layers is the number of RNN layers you want to stack, dropout is the dropout rate after each RNN layer, units are the number of RNN cell units (whether it is LSTM, SimpleRNN, or GRU), bidirectional is a boolean that indicates whether to use bidirectional RNNs, experiment with those!\n",
    "\n",
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "260916e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 15\n",
    "# whether to scale feature columns & output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "### model parameters\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "### training parameters\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 500\n",
    "# Amazon stock market\n",
    "ticker = \"AMZN\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e42998d",
   "metadata": {},
   "source": [
    "So the above code is all about defining all the hyperparameters we gonna use, we explained some of them, while we didn't on the others:\n",
    "\n",
    "TEST_SIZE: The testing set rate. For instance 0.2 means 20% of the total dataset.\n",
    "\n",
    "FEATURE_COLUMNS: The features we gonna use to predict the next price value.\n",
    "\n",
    "N_LAYERS: Number of RNN layers to use.\n",
    "\n",
    "CELL: RNN cell to use, default is LSTM.\n",
    "\n",
    "UNITS: Number of cell units.\n",
    "\n",
    "DROPOUT: The dropout rate is the probability of not training a given node in a layer, where 0.0 means no dropout at all. This type of regularization can help the model to not overfit our training data.\n",
    "\n",
    "BIDIRECTIONAL: Whether to use bidirectional recurrent neural networks.\n",
    "\n",
    "LOSS: Loss function to use for this regression problem, we're using Huber\n",
    "loss, you can use mean absolute error (mae) or mean squared error (mse) as well.\n",
    "\n",
    "OPTIMIZER: Optimization algorithm to use, defaulting to Adam.\n",
    "\n",
    "BATCH_SIZE: The number of data samples to use on each training iteration.\n",
    "\n",
    "EPOCHS: The number of times that the learning algorithm will pass through the entire training dataset, we used 500 here, but try to increase it furthermore.\n",
    "\n",
    "Feel free to experiment with these values to get better results than mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be7d8ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.19.5 in c:\\users\\msanh\\anaconda3\\lib\\site-packages (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.19.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da25d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a2b090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 1/76 [..............................] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1278WARNING:tensorflow:From C:\\Users\\msanh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0219\n",
      "Epoch 00001: val_loss improved from inf to 0.00017, saving model to results\\2021-10-03_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "76/76 [==============================] - 35s 455ms/step - loss: 0.0012 - mean_absolute_error: 0.0219 - val_loss: 1.7443e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 2/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 6.1770e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00002: val_loss improved from 0.00017 to 0.00017, saving model to results\\2021-10-03_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "76/76 [==============================] - 34s 445ms/step - loss: 6.1770e-04 - mean_absolute_error: 0.0176 - val_loss: 1.7216e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 3/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 4.4750e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00003: val_loss improved from 0.00017 to 0.00016, saving model to results\\2021-10-03_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "76/76 [==============================] - 37s 485ms/step - loss: 4.4750e-04 - mean_absolute_error: 0.0146 - val_loss: 1.5667e-04 - val_mean_absolute_error: 0.0096\n",
      "Epoch 4/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 4.1947e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00004: val_loss did not improve from 0.00016\n",
      "76/76 [==============================] - 36s 474ms/step - loss: 4.1947e-04 - mean_absolute_error: 0.0138 - val_loss: 1.7230e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 5/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 4.6447e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00005: val_loss did not improve from 0.00016\n",
      "76/76 [==============================] - 33s 439ms/step - loss: 4.6447e-04 - mean_absolute_error: 0.0144 - val_loss: 3.0649e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 6/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 4.9978e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00006: val_loss improved from 0.00016 to 0.00015, saving model to results\\2021-10-03_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "76/76 [==============================] - 35s 456ms/step - loss: 4.9978e-04 - mean_absolute_error: 0.0152 - val_loss: 1.5112e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 7/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 4.1792e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00007: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 34s 451ms/step - loss: 4.1792e-04 - mean_absolute_error: 0.0138 - val_loss: 1.7750e-04 - val_mean_absolute_error: 0.0090\n",
      "Epoch 8/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 4.1671e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00008: val_loss improved from 0.00015 to 0.00015, saving model to results\\2021-10-03_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "76/76 [==============================] - 31s 412ms/step - loss: 4.1671e-04 - mean_absolute_error: 0.0137 - val_loss: 1.5025e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 9/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 4.4390e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00009: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 33s 441ms/step - loss: 4.4390e-04 - mean_absolute_error: 0.0147 - val_loss: 3.0394e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 10/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 6.0713e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 00010: val_loss improved from 0.00015 to 0.00015, saving model to results\\2021-10-03_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "76/76 [==============================] - 33s 438ms/step - loss: 6.0713e-04 - mean_absolute_error: 0.0172 - val_loss: 1.4818e-04 - val_mean_absolute_error: 0.0078\n",
      "Epoch 11/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 5.1244e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00011: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 32s 424ms/step - loss: 5.1244e-04 - mean_absolute_error: 0.0155 - val_loss: 5.2370e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 12/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 4.0262e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00012: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 34s 444ms/step - loss: 4.0262e-04 - mean_absolute_error: 0.0141 - val_loss: 1.5102e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 13/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.8739e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00013: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 32s 427ms/step - loss: 3.8739e-04 - mean_absolute_error: 0.0136 - val_loss: 2.3667e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 14/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.6564e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00014: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 33s 429ms/step - loss: 3.6564e-04 - mean_absolute_error: 0.0134 - val_loss: 2.7323e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 15/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.8536e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00015: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 35s 461ms/step - loss: 3.8536e-04 - mean_absolute_error: 0.0136 - val_loss: 1.9530e-04 - val_mean_absolute_error: 0.0092\n",
      "Epoch 16/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.9248e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00016: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 33s 437ms/step - loss: 3.9248e-04 - mean_absolute_error: 0.0138 - val_loss: 2.0673e-04 - val_mean_absolute_error: 0.0092\n",
      "Epoch 17/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.8636e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00017: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 34s 442ms/step - loss: 3.8636e-04 - mean_absolute_error: 0.0137 - val_loss: 1.5888e-04 - val_mean_absolute_error: 0.0085\n",
      "Epoch 18/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.9631e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00018: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 35s 459ms/step - loss: 3.9631e-04 - mean_absolute_error: 0.0143 - val_loss: 2.3388e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 19/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.3897e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00019: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 35s 460ms/step - loss: 3.3897e-04 - mean_absolute_error: 0.0136 - val_loss: 1.5311e-04 - val_mean_absolute_error: 0.0078\n",
      "Epoch 20/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.5355e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00020: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 33s 439ms/step - loss: 3.5355e-04 - mean_absolute_error: 0.0135 - val_loss: 2.9915e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 21/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 4.0052e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00021: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 35s 460ms/step - loss: 4.0052e-04 - mean_absolute_error: 0.0142 - val_loss: 1.6154e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 22/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.3601e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00022: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 35s 456ms/step - loss: 3.3601e-04 - mean_absolute_error: 0.0135 - val_loss: 2.4965e-04 - val_mean_absolute_error: 0.0102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.8123e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00023: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 35s 467ms/step - loss: 3.8123e-04 - mean_absolute_error: 0.0140 - val_loss: 1.7271e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 24/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.6713e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00024: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 36s 469ms/step - loss: 3.6713e-04 - mean_absolute_error: 0.0138 - val_loss: 1.7522e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 25/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.3485e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00025: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 31s 407ms/step - loss: 3.3485e-04 - mean_absolute_error: 0.0134 - val_loss: 1.5093e-04 - val_mean_absolute_error: 0.0082\n",
      "Epoch 26/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.7133e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00026: val_loss did not improve from 0.00015\n",
      "76/76 [==============================] - 32s 417ms/step - loss: 3.7133e-04 - mean_absolute_error: 0.0139 - val_loss: 3.4728e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 27/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.3612e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00027: val_loss improved from 0.00015 to 0.00014, saving model to results\\2021-10-03_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "76/76 [==============================] - 35s 455ms/step - loss: 3.3612e-04 - mean_absolute_error: 0.0136 - val_loss: 1.4265e-04 - val_mean_absolute_error: 0.0081\n",
      "Epoch 28/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.2695e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00028: val_loss did not improve from 0.00014\n",
      "76/76 [==============================] - 33s 436ms/step - loss: 3.2695e-04 - mean_absolute_error: 0.0133 - val_loss: 2.5099e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 29/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.4528e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00029: val_loss did not improve from 0.00014\n",
      "76/76 [==============================] - 31s 409ms/step - loss: 3.4528e-04 - mean_absolute_error: 0.0136 - val_loss: 1.4635e-04 - val_mean_absolute_error: 0.0085\n",
      "Epoch 30/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.3184e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00030: val_loss did not improve from 0.00014\n",
      "76/76 [==============================] - 30s 396ms/step - loss: 3.3184e-04 - mean_absolute_error: 0.0136 - val_loss: 1.5588e-04 - val_mean_absolute_error: 0.0077\n",
      "Epoch 31/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.2189e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00031: val_loss did not improve from 0.00014\n",
      "76/76 [==============================] - 34s 442ms/step - loss: 3.2189e-04 - mean_absolute_error: 0.0133 - val_loss: 1.4917e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 32/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.2357e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00032: val_loss did not improve from 0.00014\n",
      "76/76 [==============================] - 38s 494ms/step - loss: 3.2357e-04 - mean_absolute_error: 0.0134 - val_loss: 1.9798e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 33/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.9228e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00033: val_loss did not improve from 0.00014\n",
      "76/76 [==============================] - 34s 447ms/step - loss: 2.9228e-04 - mean_absolute_error: 0.0132 - val_loss: 1.6776e-04 - val_mean_absolute_error: 0.0086\n",
      "Epoch 34/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.3413e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00034: val_loss did not improve from 0.00014\n",
      "76/76 [==============================] - 32s 417ms/step - loss: 3.3413e-04 - mean_absolute_error: 0.0137 - val_loss: 1.8786e-04 - val_mean_absolute_error: 0.0087\n",
      "Epoch 35/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.4876e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00035: val_loss did not improve from 0.00014\n",
      "76/76 [==============================] - 32s 415ms/step - loss: 3.4876e-04 - mean_absolute_error: 0.0138 - val_loss: 2.0205e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 36/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.1167e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00036: val_loss improved from 0.00014 to 0.00014, saving model to results\\2021-10-03_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "76/76 [==============================] - 33s 430ms/step - loss: 3.1167e-04 - mean_absolute_error: 0.0135 - val_loss: 1.3685e-04 - val_mean_absolute_error: 0.0073\n",
      "Epoch 37/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.2850e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00037: val_loss improved from 0.00014 to 0.00014, saving model to results\\2021-10-03_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "76/76 [==============================] - 34s 442ms/step - loss: 3.2850e-04 - mean_absolute_error: 0.0138 - val_loss: 1.3679e-04 - val_mean_absolute_error: 0.0072\n",
      "Epoch 38/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.8944e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00038: val_loss improved from 0.00014 to 0.00014, saving model to results\\2021-10-03_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "76/76 [==============================] - 33s 429ms/step - loss: 2.8944e-04 - mean_absolute_error: 0.0132 - val_loss: 1.3544e-04 - val_mean_absolute_error: 0.0074\n",
      "Epoch 39/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.1150e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00039: val_loss did not improve from 0.00014\n",
      "76/76 [==============================] - 32s 418ms/step - loss: 3.1150e-04 - mean_absolute_error: 0.0133 - val_loss: 1.9686e-04 - val_mean_absolute_error: 0.0094\n",
      "Epoch 40/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.0321e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00040: val_loss did not improve from 0.00014\n",
      "76/76 [==============================] - 32s 422ms/step - loss: 3.0321e-04 - mean_absolute_error: 0.0131 - val_loss: 1.3800e-04 - val_mean_absolute_error: 0.0080\n",
      "Epoch 41/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.2220e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00041: val_loss did not improve from 0.00014\n",
      "76/76 [==============================] - 31s 414ms/step - loss: 3.2220e-04 - mean_absolute_error: 0.0139 - val_loss: 1.6158e-04 - val_mean_absolute_error: 0.0079\n",
      "Epoch 42/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.9050e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00042: val_loss did not improve from 0.00014\n",
      "76/76 [==============================] - 31s 414ms/step - loss: 2.9050e-04 - mean_absolute_error: 0.0130 - val_loss: 1.3680e-04 - val_mean_absolute_error: 0.0073\n",
      "Epoch 43/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.9284e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00043: val_loss did not improve from 0.00014\n",
      "76/76 [==============================] - 32s 420ms/step - loss: 2.9284e-04 - mean_absolute_error: 0.0131 - val_loss: 1.6395e-04 - val_mean_absolute_error: 0.0088\n",
      "Epoch 44/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.9587e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00044: val_loss did not improve from 0.00014\n",
      "76/76 [==============================] - 32s 416ms/step - loss: 2.9587e-04 - mean_absolute_error: 0.0133 - val_loss: 1.5062e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 45/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.1150e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00045: val_loss did not improve from 0.00014\n",
      "76/76 [==============================] - 32s 425ms/step - loss: 3.1150e-04 - mean_absolute_error: 0.0136 - val_loss: 1.3846e-04 - val_mean_absolute_error: 0.0087\n",
      "Epoch 46/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.7995e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00046: val_loss improved from 0.00014 to 0.00013, saving model to results\\2021-10-03_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "76/76 [==============================] - 32s 417ms/step - loss: 2.7995e-04 - mean_absolute_error: 0.0132 - val_loss: 1.3082e-04 - val_mean_absolute_error: 0.0076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.1232e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00047: val_loss did not improve from 0.00013\n",
      "76/76 [==============================] - 32s 416ms/step - loss: 3.1232e-04 - mean_absolute_error: 0.0139 - val_loss: 1.8799e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 48/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.8270e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00048: val_loss improved from 0.00013 to 0.00013, saving model to results\\2021-10-03_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "76/76 [==============================] - 32s 427ms/step - loss: 2.8270e-04 - mean_absolute_error: 0.0132 - val_loss: 1.2688e-04 - val_mean_absolute_error: 0.0070\n",
      "Epoch 49/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.7643e-04 - mean_absolute_error: 0.0129\n",
      "Epoch 00049: val_loss did not improve from 0.00013\n",
      "76/76 [==============================] - 32s 427ms/step - loss: 2.7643e-04 - mean_absolute_error: 0.0129 - val_loss: 1.4569e-04 - val_mean_absolute_error: 0.0078\n",
      "Epoch 50/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.6103e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00050: val_loss did not improve from 0.00013\n",
      "76/76 [==============================] - 35s 460ms/step - loss: 2.6103e-04 - mean_absolute_error: 0.0127 - val_loss: 1.3177e-04 - val_mean_absolute_error: 0.0074\n",
      "Epoch 51/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.7619e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00051: val_loss did not improve from 0.00013\n",
      "76/76 [==============================] - 33s 430ms/step - loss: 2.7619e-04 - mean_absolute_error: 0.0132 - val_loss: 1.4938e-04 - val_mean_absolute_error: 0.0080\n",
      "Epoch 52/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.8199e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00052: val_loss did not improve from 0.00013\n",
      "76/76 [==============================] - 33s 437ms/step - loss: 2.8199e-04 - mean_absolute_error: 0.0131 - val_loss: 2.1235e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 53/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 3.1256e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00053: val_loss did not improve from 0.00013\n",
      "76/76 [==============================] - 33s 428ms/step - loss: 3.1256e-04 - mean_absolute_error: 0.0138 - val_loss: 1.4168e-04 - val_mean_absolute_error: 0.0088\n",
      "Epoch 54/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.7517e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00054: val_loss did not improve from 0.00013\n",
      "76/76 [==============================] - 32s 423ms/step - loss: 2.7517e-04 - mean_absolute_error: 0.0130 - val_loss: 1.6584e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 55/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.9800e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00055: val_loss did not improve from 0.00013\n",
      "76/76 [==============================] - 34s 449ms/step - loss: 2.9800e-04 - mean_absolute_error: 0.0137 - val_loss: 1.5086e-04 - val_mean_absolute_error: 0.0086\n",
      "Epoch 56/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.9284e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00056: val_loss did not improve from 0.00013\n",
      "76/76 [==============================] - 33s 433ms/step - loss: 2.9284e-04 - mean_absolute_error: 0.0135 - val_loss: 1.7538e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 57/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.7724e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00057: val_loss improved from 0.00013 to 0.00012, saving model to results\\2021-10-03_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "76/76 [==============================] - 33s 430ms/step - loss: 2.7724e-04 - mean_absolute_error: 0.0134 - val_loss: 1.2310e-04 - val_mean_absolute_error: 0.0071\n",
      "Epoch 58/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.5592e-04 - mean_absolute_error: 0.0129\n",
      "Epoch 00058: val_loss did not improve from 0.00012\n",
      "76/76 [==============================] - 32s 425ms/step - loss: 2.5592e-04 - mean_absolute_error: 0.0129 - val_loss: 1.3856e-04 - val_mean_absolute_error: 0.0073\n",
      "Epoch 59/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.5232e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00059: val_loss did not improve from 0.00012\n",
      "76/76 [==============================] - 32s 426ms/step - loss: 2.5232e-04 - mean_absolute_error: 0.0127 - val_loss: 1.5357e-04 - val_mean_absolute_error: 0.0080\n",
      "Epoch 60/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.6770e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00060: val_loss did not improve from 0.00012\n",
      "76/76 [==============================] - 32s 423ms/step - loss: 2.6770e-04 - mean_absolute_error: 0.0132 - val_loss: 1.5718e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 61/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.8520e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00061: val_loss did not improve from 0.00012\n",
      "76/76 [==============================] - 32s 425ms/step - loss: 2.8520e-04 - mean_absolute_error: 0.0133 - val_loss: 1.3067e-04 - val_mean_absolute_error: 0.0073\n",
      "Epoch 62/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.6051e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00062: val_loss did not improve from 0.00012\n",
      "76/76 [==============================] - 33s 429ms/step - loss: 2.6051e-04 - mean_absolute_error: 0.0128 - val_loss: 1.4358e-04 - val_mean_absolute_error: 0.0081\n",
      "Epoch 63/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.5717e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00063: val_loss did not improve from 0.00012\n",
      "76/76 [==============================] - 31s 414ms/step - loss: 2.5717e-04 - mean_absolute_error: 0.0130 - val_loss: 1.3622e-04 - val_mean_absolute_error: 0.0077\n",
      "Epoch 64/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.4901e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00064: val_loss did not improve from 0.00012\n",
      "76/76 [==============================] - 34s 445ms/step - loss: 2.4901e-04 - mean_absolute_error: 0.0128 - val_loss: 1.3842e-04 - val_mean_absolute_error: 0.0073\n",
      "Epoch 65/500\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.3401e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00065: val_loss did not improve from 0.00012\n",
      "76/76 [==============================] - 33s 438ms/step - loss: 2.3401e-04 - mean_absolute_error: 0.0125 - val_loss: 1.3631e-04 - val_mean_absolute_error: 0.0074\n",
      "Epoch 66/500\n",
      "12/76 [===>..........................] - ETA: 21s - loss: 2.9050e-04 - mean_absolute_error: 0.0132"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see \n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba150e09",
   "metadata": {},
   "source": [
    "We used ModelCheckpoint which saves our model in each epoch during the training. We also used TensorBoard to visualize the model performance in the training process.\n",
    "\n",
    "After running the above block of code, it will train the model for 500 epochs (as we set previously), so it will take some time, here is the first output lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=\"logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b2bb4",
   "metadata": {},
   "source": [
    "Now this will start a local HTTP server at localhost:6006, after going to the browser, you'll see something similar to this:<img source=\"https://www.thepythoncode.com/media/articles/stock-price-prediction-in-python-using-tensorflow-2-and-keras/loss-in-tensorboard.PNG\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb46b8",
   "metadata": {},
   "source": [
    "The loss is Huber loss as specified in the LOSS parameter (you can always change it to mean absolute error or mean squared error), the curve is the validation loss. As you can see, it is significantly decreasing over time, you can also increase the number of epochs to get much better results.\n",
    "\n",
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9868fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(test_df):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9917781",
   "metadata": {},
   "source": [
    "The below function takes the model and the data that was returned by create_model() and load_data() functions respectively, and constructs a dataframe in which it includes the predicted adjclose along with true future adjclose, as well as calculating buy and sell profit, we'll see it in action in a moment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93781c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    \"\"\"\n",
    "    This function takes the `model` and `data` dict to \n",
    "    construct a final dataframe that includes the features along \n",
    "    with true and predicted prices of the testing dataset\n",
    "    \"\"\"\n",
    "    # if predicted future price is higher than the current, \n",
    "    # then calculate the true future price minus the current price, to get the buy profit\n",
    "    buy_profit  = lambda current, pred_future, true_future: true_future - current if pred_future > current else 0\n",
    "    # if the predicted future price is lower than the current price,\n",
    "    # then subtract the true future price from the current price\n",
    "    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6859edca",
   "metadata": {},
   "source": [
    "The last function we gonna define is the one that's responsible for predicting the next future price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b561cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc875349",
   "metadata": {},
   "source": [
    "Now that we have the necessary functions for evaluating our model, let's load the optimal weights and proceed with evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf19a13",
   "metadata": {},
   "source": [
    "Calculating loss and mean absolute error using model.evaluate() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c270ceb",
   "metadata": {},
   "source": [
    "We also take scaled output values into consideration, so we use the inverse_transform() method from the MinMaxScaler we defined in the load_data() function earlier if the SCALE parameter was set to True.\n",
    "\n",
    "Now let's call the get_final_df() function we defined earlier to construct our testing set dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final dataframe for the testing set\n",
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b12363",
   "metadata": {},
   "source": [
    "Also, let's use predict() function to get the future price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1bfe07",
   "metadata": {},
   "source": [
    "The below code calculates the accuracy score by counting the number of positive profits (in both buy profit and sell profit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2563d5a",
   "metadata": {},
   "source": [
    "We also calculate profit per trade which is essentially the total profit divided by the number of testing samples. Printing all the previously calculated metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a15e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8815da6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
